# ğŸŒ¤ï¸ Weather Data Pipeline com Apache Airflow

Este projeto implementa um pipeline de dados automatizado usando **Apache Airflow**, que coleta dados de previsÃ£o do tempo de uma API pÃºblica, processa essas informaÃ§Ãµes e armazena em um banco de dados **PostgreSQL**.

O objetivo Ã© demonstrar habilidades essenciais para a Ã¡rea de **Engenharia de Dados**, como orquestraÃ§Ã£o de workflows, integraÃ§Ã£o com APIs, ETL com Python e persistÃªncia de dados em banco relacional.

---

## ğŸ§° Tecnologias Utilizadas

- [Apache Airflow](https://airflow.apache.org/)
- [Python 3.9+](https://www.python.org/)
- [PostgreSQL](https://www.postgresql.org/)
- [Docker & Docker Compose](https://www.docker.com/)
- [OpenWeather API](https://openweathermap.org/api)

---

## ğŸ› ï¸ Funcionalidades

- â±ï¸ Pipeline agendado com Airflow
- ğŸŒ Coleta de dados via API pÃºblica
- ğŸ§¹ TransformaÃ§Ã£o e limpeza dos dados com Python
- ğŸ—ƒï¸ Armazenamento dos dados no PostgreSQL
- ğŸ“§ (Opcional) Envio de alerta por e-mail caso previsÃ£o seja de chuva

---

## âš™ï¸ Como Executar o Projeto

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/weather-data-pipeline-airflow.git
cd weather-data-pipeline-airflow

2. Configure o arquivo .env

Crie um arquivo .env na raiz com o seguinte conteÃºdo:

WEATHER_API_KEY=sua_api_key_aqui
CITY=SÃ£o Paulo

    ğŸ”‘ Para obter a API Key, cadastre-se gratuitamente em https://openweathermap.org/api

3. Suba os containers com Docker

docker-compose up -d

    Airflow Web UI: http://localhost:8080
    Login padrÃ£o:

        UsuÃ¡rio: admin

        Senha: admin

    PostgreSQL rodando na porta 5432

ğŸ“‚ Estrutura do Projeto

weather-data-pipeline-airflow/
â”œâ”€â”€ dags/                    # Arquivos da DAG do Airflow
â”‚   â””â”€â”€ weather_dag.py
â”œâ”€â”€ scripts/                 # Scripts Python de ETL
â”‚   â”œâ”€â”€ fetch_weather.py
â”‚   â”œâ”€â”€ transform_weather.py
â”‚   â””â”€â”€ load_to_postgres.py
â”œâ”€â”€ data/                    # (opcional) backups locais
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ docker-compose.yml       # Ambiente Docker
â””â”€â”€ README.md

ğŸ“Œ Objetivo

Este projeto foi desenvolvido como parte do meu aprendizado prÃ¡tico para atuar como Engenheiro de Dados JÃºnior, com foco em pipelines de dados, automaÃ§Ã£o e boas prÃ¡ticas de orquestraÃ§Ã£o com Airflow.
âœ¨ Melhorias Futuras

    Dashboard com Streamlit para visualizaÃ§Ã£o dos dados

    Deploy em nuvem (ex: AWS EC2 + RDS)

    Uso de Sensor no Airflow para aguardar condiÃ§Ãµes especÃ­ficas

    AdiÃ§Ã£o de testes unitÃ¡rios nos scripts

ğŸ‘¨â€ğŸ’» Autor

Caio
